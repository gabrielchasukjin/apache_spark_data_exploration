{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4112e7",
   "metadata": {},
   "source": [
    "# Exploration and Feature Engineering using Amazon Dataset in Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202dfa49",
   "metadata": {},
   "source": [
    "**Apache Spark**, distributed computing system, renowned for its performance in big data processing. Spark's in-memory computing power significantly speeds up data processing tasks, particularly for iterative operations common in machine learning. \n",
    "\n",
    "Its user-friendly nature is enhanced by high-level APIs in languages like Java, Scala, Python, and R, making Spark accessible to a broader audience of developers and data scientists. The framework simplifies the development of complex parallel applications and offers built-in modules for various tasks, including SQL, streaming data, machine learning, and graph processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a936e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "# Boiler plates\n",
    "# Establishing Cluster on DSMLP Data Science and Machine Learning Platform \n",
    "PID = 'A17017372' \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from utilities import SEED\n",
    "from utilities import PA2Test\n",
    "from utilities import PA2Data\n",
    "from utilities import data_cat\n",
    "from pa2_main import PA2Executor\n",
    "import time\n",
    "\n",
    "import pyspark.ml as M\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77005a0c",
   "metadata": {},
   "source": [
    "**SparkSession**, entry point for working with DataFrame and SQL functionality in Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7262a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/28 01:26:57 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "23/12/28 01:27:57 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "23/12/28 01:27:57 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "23/12/28 01:27:57 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n",
      "23/12/28 01:27:57 ERROR AsyncEventQueue: Listener AppStatusListener threw an exception\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:192)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "23/12/28 01:27:57 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:650)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:650)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     test_results_root \u001b[38;5;241m=\u001b[39m data_cat\u001b[38;5;241m.\u001b[39mtest_results_root\n\u001b[1;32m     15\u001b[0m     pid \u001b[38;5;241m=\u001b[39m PID\n\u001b[0;32m---> 17\u001b[0m pa2 \u001b[38;5;241m=\u001b[39m \u001b[43mPA2Executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_FORMAT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m data_io \u001b[38;5;241m=\u001b[39m pa2\u001b[38;5;241m.\u001b[39mdata_io\n\u001b[1;32m     19\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m pa2\u001b[38;5;241m.\u001b[39mdata_dict\n",
      "File \u001b[0;32m~/private/dsc102_pa2/DSC102-PA2-FA23/src/pa2_main.py:24\u001b[0m, in \u001b[0;36mPA2Executor.__init__\u001b[0;34m(self, args, task_imls, input_format, synonmys, output_pid_folder)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     17\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     output_pid_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m ):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark \u001b[38;5;241m=\u001b[39m \u001b[43mspark_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkoalas\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     26\u001b[0m         ks\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute.default_index_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistributed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/private/dsc102_pa2/DSC102-PA2-FA23/src/utilities.py:246\u001b[0m, in \u001b[0;36mspark_init\u001b[0;34m(pid)\u001b[0m\n\u001b[1;32m    243\u001b[0m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.replClassServer.port\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m60004\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.authenticate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 246\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SQLContext(spark)\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/context.py:282\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/context.py:402\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:650)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--py-files utilities.py,assignment2.py \\\n",
    "--deploy-mode client \\\n",
    "pyspark-shell'\n",
    "\n",
    "INPUT_FORMAT = 'dataframe'\n",
    "\n",
    "class args:\n",
    "    review_filename = data_cat.review_filename\n",
    "    product_filename = data_cat.product_filename\n",
    "    product_processed_filename = data_cat.product_processed_filename\n",
    "    ml_features_train_filename = data_cat.ml_features_train_filename\n",
    "    ml_features_test_filename = data_cat.ml_features_test_filename\n",
    "    output_root = '/home/{}/{}-pa2/test_results'.format(getpass.getuser(), PID)\n",
    "    test_results_root = data_cat.test_results_root\n",
    "    pid = PID\n",
    "\n",
    "pa2 = PA2Executor(args, input_format=INPUT_FORMAT)\n",
    "data_io = pa2.data_io\n",
    "data_dict = pa2.data_dict\n",
    "\n",
    "# Bring the part_1 datasets to memory and de-cache part_2 datasets. \n",
    "# Execute this once before you start working on this Part\n",
    "data_dict, _ = data_io.cache_switch(data_dict, 'part_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750116f8",
   "metadata": {},
   "source": [
    "## DataFrame Library \n",
    "\n",
    "#### Python API for Apache Spark\n",
    "\n",
    "Module (F) contains a variety of functions for manipulating data in DataFrames. These functions include string manipulation, date arithmetic, common mathematical operations, and more complex operations like window functions.\n",
    "\n",
    "Module (M) represents Spark's machine learning library. It provides a variety of machine learning algorithms, feature transformers, and utilities for constructing ML pipelines, training models, and making predictions.\n",
    "\n",
    "Module (T) provides classes that describe the types of data in DataFrames, like string, integer, float, array, and more complex types like StructType. These types are used to define the schema of DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e090823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml as M\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import col, avg, count, udf, when, size, lit, explode\n",
    "from pyspark.sql.functions import map_keys, map_values, lower, split\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c0a5b9",
   "metadata": {},
   "source": [
    "# DataSet Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06fe6a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- salesRank: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- related: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_data = data_dict['product']\n",
    "product_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c8aa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of products in dataset: 9430000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of products in dataset: \" + str(product_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16beb179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- overall: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_data = data_dict['review']\n",
    "review_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecec7fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews in dataset: 57873997\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of reviews in dataset: \" + str(review_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb635b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_processed_data = data_dict['product_processed']\n",
    "product_processed_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10bfb7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of products in processed dataset: 9430000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of products in processed dataset: \" + str(product_processed_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f2910",
   "metadata": {},
   "source": [
    "## Merging Reviews with Product Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22250995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_df = product_data.join(review_data, \"asin\", \"left\")\n",
    "    \n",
    "ratings_df = joined_df.groupBy(\"asin\").agg(\n",
    "    avg(\"overall\").alias(\"meanRating\"),\n",
    "    count(\"overall\").alias(\"countRating\")\n",
    ")\n",
    "\n",
    "summary_stats = ratings_df.describe(\"meanRating\", \"countRating\").toPandas()\n",
    "\n",
    "numZeros_countRating = ratings_df.filter(col(\"countRating\") == 0).count()\n",
    "\n",
    "filtered_df = ratings_df.filter(col(\"countRating\") != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375794ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>meanRating</th>\n",
       "      <th>countRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>7456220</td>\n",
       "      <td>9430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>4.151314482625535</td>\n",
       "      <td>5.885972852598091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1.0628905260845993</td>\n",
       "      <td>45.849015609871294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary          meanRating         countRating\n",
       "0   count             7456220             9430000\n",
       "1    mean   4.151314482625535   5.885972852598091\n",
       "2  stddev  1.0628905260845993  45.849015609871294\n",
       "3     min                 1.0                   0\n",
       "4     max                 5.0               17817"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20479645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------+\n",
      "|      asin|        meanRating|countRating|\n",
      "+----------+------------------+-----------+\n",
      "|0000032034|3.6666666666666665|          3|\n",
      "|0000095699|               3.0|          1|\n",
      "|0000143502|              null|          0|\n",
      "|0000143529|              null|          0|\n",
      "|0001048775|              null|          0|\n",
      "|0001064487| 4.666666666666667|          3|\n",
      "|0001203088|               5.0|          1|\n",
      "|0001380877|               5.0|          1|\n",
      "|0001384163|               5.0|          1|\n",
      "|0001384198|               5.0|          1|\n",
      "|0001720252|               2.0|          1|\n",
      "|0001837192|              null|          0|\n",
      "|0001845357|               4.0|          1|\n",
      "|0001850164|              3.75|          4|\n",
      "|0001856871|               3.0|          1|\n",
      "|0001857169|               5.0|          1|\n",
      "|0001923331|               5.0|          1|\n",
      "|0001956752|               5.0|          1|\n",
      "|000195783X|              null|          0|\n",
      "|0001982796|               5.0|          2|\n",
      "+----------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca220e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973780 products do not have any reviews.\n"
     ]
    }
   ],
   "source": [
    "print(str(numZeros_countRating) + \" products do not have any reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f356e93",
   "metadata": {},
   "source": [
    "## Flattening  Categories [Array of Arrays] into Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33623382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_category = product_data.withColumn(\n",
    "        \"category\",\n",
    "        F.when(F.size(F.col(\"categories\")) > 0, F.col(\"categories\")[0][0])\n",
    "        .otherwise(F.lit(None))\n",
    "    )\n",
    "    \n",
    "df_with_category = df_with_category.withColumn(\"category\", F.when(F.col(\"category\") != \"\", F.col(\"category\")).otherwise(F.lit(None)))\n",
    "\n",
    "df_with_cat_sales = df_with_category.withColumn(\n",
    "    \"bestSalesCategory\",\n",
    "    when(col(\"salesRank\").isNull(), lit(None))\n",
    "    .otherwise(map_keys(col(\"salesRank\"))[0])\n",
    ").withColumn(\n",
    "    \"bestSalesRank\",\n",
    "    when(col(\"salesRank\").isNull(), lit(None))\n",
    "    .otherwise(map_values(col(\"salesRank\"))[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcf30cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      asin|            category|\n",
      "+----------+--------------------+\n",
      "|B00I8HVV6E|      Home & Kitchen|\n",
      "|B00I8KEOTM|    Apps for Android|\n",
      "|B00I8KCW4G|Clothing, Shoes &...|\n",
      "|B00I8JKCQW|Clothing, Shoes &...|\n",
      "|B00I8JKI8E|Clothing, Shoes &...|\n",
      "|B00I8JEVO6|   Sports & Outdoors|\n",
      "|B00I8K312I|              Beauty|\n",
      "|B00I8K9A3M|               Books|\n",
      "|B00I8KDKHO|          Automotive|\n",
      "|B00I8JGBIU|        Toys & Games|\n",
      "|B00I8H5FQG|Amazon Instant Video|\n",
      "|B00I8KGXLY|Clothing, Shoes &...|\n",
      "|B00I8JPBB8|      Home & Kitchen|\n",
      "|B00I8KFRL6|Health & Personal...|\n",
      "|B00I8IU9QQ|    Apps for Android|\n",
      "|B00I8KF7ZW|Clothing, Shoes &...|\n",
      "|B00I8JMBKM|Clothing, Shoes &...|\n",
      "|B00I8I50EM|               Books|\n",
      "|B00I8JEHJ0|Cell Phones & Acc...|\n",
      "|B00I8JIDJ0|Clothing, Shoes &...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_category.select(col(\"asin\"), col(\"category\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5538a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter out rows where 'category' is null\n",
    "df_filtered = df_with_cat_sales.filter(F.col(\"category\").isNotNull())\n",
    "\n",
    "# Now count distinct values in the 'category' column\n",
    "countDistinct_category = df_filtered.select(\"category\").distinct().count()\n",
    "\n",
    "# Count nulls in bestSalesCategory\n",
    "numNulls_bestSalesCategory = df_with_cat_sales.filter(F.col(\"bestSalesCategory\").isNull()).count()\n",
    "\n",
    "df_filtered_sales_cat = df_with_cat_sales.filter(F.col(\"bestSalesCategory\").isNotNull())\n",
    "\n",
    "# Count distinct values in bestSalesCategory, excluding nulls\n",
    "countDistinct_bestSalesCategory = df_filtered_sales_cat.select(\"bestSalesCategory\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82d6af22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 distinct categories in products dataset.\n"
     ]
    }
   ],
   "source": [
    "print(str(countDistinct_category) + \" distinct categories in products dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7cac80",
   "metadata": {},
   "source": [
    "## Flattening Related key-value Pair into Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1eb21",
   "metadata": {},
   "source": [
    "Each entry of `related` column is a map with four keys/attributes: `also_bought`, `also_viewed`, `bought_together`,and `buy_after_viewing`. Each value of these keys contains an array of product IDs. We call them attribute arrays.\n",
    "\n",
    "Goal: Calculate the mean price of all products from the `also_viewed` attribute array, and place it into a new column `menaPriceAlsoViewed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "861b1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    " # Inputs:\n",
    "asin_column = 'asin'\n",
    "price_column = 'price'\n",
    "attribute = 'also_viewed'\n",
    "related_column = 'related'\n",
    "# Outputs:\n",
    "meanPriceAlsoViewed_column = 'meanPriceAlsoViewed'\n",
    "countAlsoViewed_column = 'countAlsoViewed'\n",
    "\n",
    "# Define column references from the DataFrame\n",
    "asin_col = F.col(asin_column)  # Reference to the 'asin' column\n",
    "price_col = F.col(price_column)  # Reference to the 'price' column\n",
    "attribute_col = F.col(attribute)  # Reference to the 'attribute' column\n",
    "related_col = F.col(related_column)  # Reference to the 'related' column\n",
    "\n",
    "# Explode the 'related' column to create a new row for each element in the list/map\n",
    "data_exploded = product_data.select(\n",
    "    asin_col,\n",
    "    F.explode(related_col.getItem(attribute)).alias('asinSK')\n",
    ").alias('data_exploded')\n",
    "\n",
    "# Join the exploded data with the original product data on matching ASIN values\n",
    "data_joined = data_exploded.join(\n",
    "    product_data.alias('product_data'),\n",
    "    F.col('data_exploded.asinSK') == F.col('product_data.asin'),\n",
    "    how='left'\n",
    ").drop(F.col('product_data.asin'))  # Drop the duplicate 'asin' column from the join result\n",
    "\n",
    "# Aggregate data by ASIN to calculate mean price and count of also viewed products\n",
    "data_agg = data_joined.groupBy(asin_column).agg(\n",
    "    F.mean(F.when(price_col.isNotNull(), price_col)).alias(meanPriceAlsoViewed_column),\n",
    "    F.count(F.col('asinSK')).alias(countAlsoViewed_column)\n",
    ")\n",
    "\n",
    "# Join the aggregated data back to the original product data\n",
    "product_data_output = product_data.join(data_agg, [asin_column], how='left')\n",
    "\n",
    "# Prepare functions for further aggregation - mean, variance, and count of nulls\n",
    "fns = []\n",
    "for column in [meanPriceAlsoViewed_column, countAlsoViewed_column]:\n",
    "    fns.append(F.mean(F.when(F.col(column).isNotNull(), F.col(column))).alias(f'mean_{column}'))\n",
    "    fns.append(F.variance(F.when(F.col(column).isNotNull(), F.col(column))).alias(f'variance_{column}'))\n",
    "    fns.append(F.sum(F.col(column).isNull().cast('integer')).alias(f'numNulls_{column}'))\n",
    "\n",
    "# Apply the prepared functions and collect the first row of the result\n",
    "rr = product_data_output.select(fns).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ba65753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 118:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------+\n",
      "|      asin|meanPriceAlsoViewed|countAlsoViewed|\n",
      "+----------+-------------------+---------------+\n",
      "|0195338103|               null|           null|\n",
      "|0465014984|               null|           null|\n",
      "|0465015638|               null|           null|\n",
      "|0465016073|               null|           null|\n",
      "|0465016537|               null|           null|\n",
      "|0806525312|  12.92333337995741|             11|\n",
      "|0883686341|               null|           null|\n",
      "|0883686600|               null|           null|\n",
      "|1590950305|               null|           null|\n",
      "|1844487229|               null|           null|\n",
      "|3540041303|               null|           null|\n",
      "|3540211322|               null|           null|\n",
      "|3540221891|               null|              1|\n",
      "|B00000DC10|               null|           null|\n",
      "|B00000DC4R| 13.861428601401192|             10|\n",
      "|B00005U14C|               null|           null|\n",
      "|B000A4HAFE|               null|           null|\n",
      "|B000E7DMU4| 25.982000732421874|              5|\n",
      "|B000H2MLLW|               null|           null|\n",
      "|B000H2MWNE|               null|           null|\n",
      "+----------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "product_data_output.select(col(\"asin\"),col(\"meanPriceAlsoViewed\"),col(\"countAlsoViewed\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387a4a5",
   "metadata": {},
   "source": [
    "## Mean/Median Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4557d",
   "metadata": {},
   "source": [
    "There are lots of nulls in the table. We will impute them with meaningful values that can be used to train machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1622402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "meanImputedPrice_column = 'meanImputedPrice'\n",
    "medianImputedPrice_column = 'medianImputedPrice'\n",
    "unknownImputedTitle_column = 'unknownImputedTitle'\n",
    "\n",
    "\n",
    "# Cast the 'price' column to float type\n",
    "product_data = product_data.withColumn(\"price\", product_data[\"price\"].cast(\"float\"))\n",
    "\n",
    "#### Impute Means\n",
    "# Calculate the mean of non-null values in the 'price' column\n",
    "mean_price = product_data.select(avg(col(\"price\")).alias(\"mean\")).collect()[0][\"mean\"]\n",
    "\n",
    "# Impute null values in the 'price' column with the mean value\n",
    "product_data = product_data.withColumn(\"meanImputedPrice\", when(col(\"price\").isNull(), mean_price).otherwise(col(\"price\")))\n",
    "\n",
    "\n",
    "#### Impute Medians\n",
    "# Calculate median\n",
    "median_price = product_data.approxQuantile(price_column, [0.5], 0.001)[0]\n",
    "# Impute null values with median\n",
    "product_data = product_data.withColumn(medianImputedPrice_column, F.when(F.col(price_column).isNull(), median_price).otherwise(F.col(price_column)))\n",
    "\n",
    "\n",
    "# Impute nulls and empty strings in the 'title' column\n",
    "product_data = product_data.withColumn(\n",
    "    unknownImputedTitle_column,\n",
    "    F.when(\n",
    "        (F.col(\"title\").isNull()) | (F.col(\"title\") == ''), F.lit('unknown')\n",
    "    ).otherwise(F.col(\"title\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0df6868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|      asin|  meanImputedPrice|medianImputedPrice|\n",
      "+----------+------------------+------------------+\n",
      "|B00I8HVV6E|27.989999771118164|27.989999771118164|\n",
      "|B00I8KEOTM| 34.93735609456491|14.989999771118164|\n",
      "|B00I8KCW4G| 41.95000076293945| 41.95000076293945|\n",
      "|B00I8JKCQW| 34.93735609456491|14.989999771118164|\n",
      "|B00I8JKI8E|24.989999771118164|24.989999771118164|\n",
      "|B00I8JEVO6| 34.93735609456491|14.989999771118164|\n",
      "|B00I8K312I|              58.0|              58.0|\n",
      "|B00I8K9A3M| 34.93735609456491|14.989999771118164|\n",
      "|B00I8KDKHO| 34.93735609456491|14.989999771118164|\n",
      "|B00I8JGBIU| 5.489999771118164| 5.489999771118164|\n",
      "|B00I8H5FQG| 34.93735609456491|14.989999771118164|\n",
      "|B00I8KGXLY|              30.0|              30.0|\n",
      "|B00I8JPBB8| 34.93735609456491|14.989999771118164|\n",
      "|B00I8KFRL6|15.989999771118164|15.989999771118164|\n",
      "|B00I8IU9QQ| 34.93735609456491|14.989999771118164|\n",
      "|B00I8KF7ZW| 34.93735609456491|14.989999771118164|\n",
      "|B00I8JMBKM| 34.93735609456491|14.989999771118164|\n",
      "|B00I8I50EM| 3.990000009536743| 3.990000009536743|\n",
      "|B00I8JEHJ0|19.989999771118164|19.989999771118164|\n",
      "|B00I8JIDJ0| 9.550000190734863| 9.550000190734863|\n",
      "+----------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_data.select(col(\"asin\"), col(\"meanImputedPrice\"), col(\"medianImputedPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd80bc5",
   "metadata": {},
   "source": [
    "## Embed `title` with word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb0319",
   "metadata": {},
   "source": [
    "I transform `title` into a fixed-length vector via word2vec which captures semantic relationships between words by translating words into numerical vectors. Words with similar meanings have vectors that are close to each other in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "502622bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/23 22:29:39 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/12/23 22:29:39 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "[Row(word='solos', similarity=0.9661991596221924), Row(word='instrumental', similarity=0.9660047888755798), Row(word='orchestra,', similarity=0.9640913605690002), Row(word='suites', similarity=0.9631103873252869), Row(word='quintet', similarity=0.9624511003494263), Row(word='transcriptions', similarity=0.95783931016922), Row(word='organ', similarity=0.9576882719993591), Row(word='accompaniment', similarity=0.9571981430053711), Row(word='mozart,', similarity=0.9547368288040161), Row(word='chopin:', similarity=0.9536173939704895)]\n"
     ]
    }
   ],
   "source": [
    "# converts the title into array list \n",
    "product_processed_data = product_processed_data.withColumn(\n",
    "    \"titleArray\", split(lower(col(\"title\")), \" \")\n",
    ")\n",
    "\n",
    "# configure the word2vec model \n",
    "\n",
    "word2Vec = Word2Vec(\n",
    "    vectorSize=16,  # Dimension of the word embedding\n",
    "    minCount=100,   # Minimum number of times a token must appear\n",
    "    numPartitions=4,  # Number of partitions\n",
    "    seed=102,       # Random seed\n",
    "    inputCol=\"titleArray\",  # Input column\n",
    "    outputCol=\"wordVectors\"  # Output column\n",
    ")\n",
    "\n",
    "model = word2Vec.fit(product_processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a752f777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bigfoot', 0.9636884927749634),\n",
       " ('ghost', 0.9486820697784424),\n",
       " ('dead', 0.9330147504806519),\n",
       " ('mccoy', 0.9303552508354187),\n",
       " ('man', 0.9302895665168762),\n",
       " ('bighorn', 0.9282985329627991),\n",
       " ('masked', 0.9274529218673706),\n",
       " ('soldier', 0.9248751997947693),\n",
       " ('chase', 0.9230467677116394),\n",
       " ('rising', 0.9200738668441772)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.findSynonymsArray('mad', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
